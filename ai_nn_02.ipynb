{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise 04/02 - Neural Networks\n",
    "\n",
    "Contact: Dr. David C. Schedl <br/>\n",
    "E-Mail: david.schedl@fh-hagenberg.at <br/>\n",
    "Note: this tutorial is geared towards students **experienced in programming** and aims to introduce you to **Backpropagation and NNs**.\n",
    "\n",
    "*Acknowledgements:*\n",
    "This notebook is strongly inspired by the NN-zero-to-here lecture series by Andrej Karpathy, available [here](https://github.com/karpathy/nn-zero-to-hero).\n",
    "\n",
    "\n",
    "## Setup\n",
    "As first step, we need to import the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and import of libraries\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs, make_circles\n",
    "\n",
    "\n",
    "# Let's set the random seed to make this reproducible (the same for everybody).\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Helpers\n",
    "\n",
    "The following functions will help us to visualize the compute graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{%sdata %.4f | grad %.4f}\" % (f\"{n.label} |\" if n.label else '', n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "This is where the magic happens ... ‚ú® <br>\n",
    "\n",
    "Let's finish backpropagation by implementing the chain rule in our `Value` class. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  \n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None # This is new!\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    # TODO: implement the backward pass for addition\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    \n",
    "    # TODO: implement the backward pass for multiplication\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    # TODO: implement the backward pass for **\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "    return self * other**-1\n",
    "\n",
    "  def __neg__(self): # -self\n",
    "    return self * -1\n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "    return self + other\n",
    "\n",
    "  # TODO: TASK 3 implement tanh\n",
    "  def tanh(self):\n",
    "    # throw NotImplementedError()\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "    out = None \n",
    "    return out  \n",
    "\n",
    "  def relu(self):\n",
    "    out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "    # TODO: implement the backward pass for ReLU\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self, ), 'exp')\n",
    "    \n",
    "    # TODO: implement the backward pass for exp\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  \n",
    "  def backward(self):\n",
    "    \n",
    "    # get a topographic sort of the graph for backprop\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self)\n",
    "\n",
    "    # set the gradient of all nodes to 0\n",
    "    for n in topo:\n",
    "      n.grad = 0.0\n",
    "    \n",
    "    self.grad = 1.0 # set the gradient of the last node to 1\n",
    "\n",
    "    # TODO: finish the backward pass!\n",
    "\n",
    "\n",
    "# We want to compute the gradients by calling backward()!\n",
    "\n",
    "x = Value(-2.0); x.label='x'\n",
    "y = Value(5); y.label='y'\n",
    "z = Value(-4.0); z.label='z'\n",
    "\n",
    "q = x + y; q.label = 'q (x+y)'\n",
    "o = q*z; o.label='o'\n",
    "\n",
    "o.backward() # THE magic (should) happen here!\n",
    "\n",
    "# gradients\n",
    "# o.grad = 1.0\n",
    "# q.grad = -4.0\n",
    "# x.grad = -4.0\n",
    "# y.grad = -4.0\n",
    "# z.grad = 3.0\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop Task 1 (previously by hand)\n",
    "The calculations you did by hand in the previous task are now implemented in the `backward` method of the `Value` class. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your birthday here (will be used to generate random numbers)\n",
    "d, m, y = 29, 2, 1980\n",
    "\n",
    "# init the random number generator\n",
    "import random\n",
    "random.seed(d+m+y); r_range = (1, 9)\n",
    "\n",
    "x1 = Value(random.randint(*r_range)); x1.label='x1'\n",
    "x2 = Value(random.randint(*r_range)); x2.label='x2'\n",
    "x3 = Value(random.randint(*r_range)); x3.label='x3'\n",
    "w1 = Value(random.randint(*r_range)); w1.label='w1'\n",
    "w2 = Value(random.randint(*r_range)); w2.label='w2'\n",
    "w3 = Value(random.randint(*r_range)); w3.label='w3'\n",
    "b = Value(random.randint(*r_range));  b.label='b'\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label='x1w1'\n",
    "x2w2 = x2*w2; x2w2.label='x2w2'\n",
    "x3w3 = x3*w3; x3w3.label='x3w3'\n",
    "sums = (x1w1 + x2w2) + (x3w3 + b); sums.label='sum'\n",
    "o = sums * Value(0.25); o.label='o'\n",
    "o.label='o'\n",
    "\n",
    "\n",
    "# Backprop do your thing!\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Example with an Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "# ---- this is the tanh activation function \n",
    "# Todo: replace it with a tanh function in Task 3\n",
    "e = (2*n).exp()\n",
    "o = (e - 1) / (e + 1)\n",
    "# ----\n",
    "o.label = 'o'\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch does the same!\n",
    "\n",
    "Let's look at the example above implemented in PyTorch. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('---')\n",
    "print('x2', x2.grad.item())\n",
    "print('w2', w2.grad.item())\n",
    "print('x1', x1.grad.item())\n",
    "print('w1', w1.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 üìù: Implement a tanh activation function\n",
    "\n",
    "Our Value class is missing a tanh activation function. Implement it and test it with the network above. <br>\n",
    "The forward and backward pass should yield the same result after the changes:\n",
    "\n",
    "| old code | new code |\n",
    "| --- | --- |\n",
    "|`e = (2*n).exp()` <br> `o = (e - 1) / (e + 1)` | `o = n.tanh()` |\n",
    "\n",
    "The gradient of the tanh function $f(x) = tanh(x)$ is given by: <br>\n",
    "$$\n",
    "\\frac{\\partial f(x)}{\\partial x} = 1 - f(x)^2 .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Let's get serious now! <br>\n",
    "With our Value class we will implement Neurons and a Multi-Layer Perceptron (MLP) with Layers. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Neuron:\n",
    "  \n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # TODO: get the Neuron working\n",
    "    pass\n",
    "  \n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "  \n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # TODO: get the Layer working\n",
    "    pass\n",
    "\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "  \n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # TODO: get the MLP working\n",
    "    pass\n",
    "\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the MLP with a single 3D input like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "n(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we will use multiple inputs, so let's create an array of inputs and desired targets (e.g. for classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compute a loss function to evaluate the performance of our network. <br>\n",
    "Furthermore, the loss is the basis for backpropagation. <br>\n",
    "\n",
    "ü§î What happens if you change the learning rate or the number of iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "N = 20 # iterations\n",
    "\n",
    "for k in range(N):\n",
    "  \n",
    "  # forward pass\n",
    "  ypred = [n(x) for x in xs]\n",
    "  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "  \n",
    "  # backward pass\n",
    "  for p in n.parameters():\n",
    "    p.grad = 0.0\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  for p in n.parameters():\n",
    "    p.data += -learning_rate * p.grad\n",
    "  \n",
    "  print(k, loss.data)\n",
    "  \n",
    "print( \"========\" )\n",
    "print( \"predictions: \" + str(ypred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train a network to classify points (like Tensorflow Playground)\n",
    "\n",
    "We will at first generate some data using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make up a dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "\n",
    "y = y*2 - 1 # make y be -1 or 1\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n",
    "plt.show()\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model \n",
    "model = MLP(2, [16, 16, 1]) # 2-layer neural network\n",
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(batch_size=None):\n",
    "    \n",
    "    # TODO: use mini-batch\n",
    "    Xb, yb = X, y\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    total_loss = data_loss + reg_loss\n",
    "    \n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "# optimization\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward\n",
    "    total_loss, acc = loss(batch_size=batch_size)\n",
    "    \n",
    "    # backward\n",
    "    #model.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    learning_rate = 1.0 - 0.9*k/100\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize decision boundary (similar to tensorflow playground)\n",
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s.data > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in PyTorch\n",
    "\n",
    "Let's also set up the same MLP as above in PyTorch. <br>\n",
    "\n",
    "Previously our model was defined in code as:\n",
    "```python\n",
    "model = MLP(2, [16, 16, 1]) # 2-layer neural network\n",
    "```\n",
    "Our MLP had 337 parameters. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP in PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TorchMLP(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = torch.tanh(self.fc1(x))\n",
    "        h2 = torch.tanh(self.fc2(h1))\n",
    "        o = self.fc3(h2)\n",
    "        return o\n",
    "\n",
    "torch_model = TorchMLP()\n",
    "\n",
    "print(torch_model)\n",
    "# print number of parameters\n",
    "print(sum(p.numel() for p in torch_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial accuracy (RANDOM)\n",
    "preds = torch_model(torch.Tensor(X)).data.numpy()\n",
    "accuracy = [(yi > 0) == (scorei > 0) for yi, scorei in zip(y, preds)]\n",
    "acc = sum(accuracy) / len(accuracy)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft marigin loss \n",
    "torch_loss_fun = F.soft_margin_loss # NOTE: this is not exactly the same as the one above, but it's close ;)\n",
    "optimizer = torch.optim.SGD(torch_model.parameters(), \n",
    "    lr=5, # learning rate\n",
    "    weight_decay=1e-4 # L2 regularization\n",
    ")\n",
    "batch_size = 100\n",
    "\n",
    "# optimization\n",
    "for k in range(100):\n",
    "\n",
    "    # use mini-batch\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    # forward\n",
    "    torch_scores = torch_model(torch.from_numpy(Xb).float())\n",
    "    torch_loss = torch_loss_fun(torch_scores, torch.from_numpy(yb).reshape(-1,1).float())\n",
    "    # L2 regularization is in the optimizer, now!!\n",
    "    \n",
    "    # backward\n",
    "    torch_loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data.item() > 0) for yi, scorei in zip(yb, torch_scores)]\n",
    "    acc = sum(accuracy) / len(accuracy)\n",
    "\n",
    "    if k % 10 == 0:\n",
    "        print(f\"step {k} loss {torch_loss.data}, accuracy {acc*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize decision boundary (similar to tensorflow playground)\n",
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "scores = torch_model(torch.from_numpy(Xmesh).float()).data.numpy()\n",
    "Z = np.array([s > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 üìù: Play with the data and the network\n",
    " \n",
    "Change the MLP (number of layers and number of neurons per layer) and see how it affects the decision boundary. \n",
    "Switch to the `make_circles` dataset (instead of the `make_moons` function) and see how your network performs. <br>\n",
    "\n",
    "Try to answer the following questions:\n",
    "- Is there any overfitting? How would you see it? <br>\n",
    "- How many parameters do the networks have and how does that affect performance (accuracy and timing)?\n",
    "- What is the simplest and most complex network you can train to classify the data? Can it get too complex?\n",
    "\n",
    "Hints: You can use our Value class for computations or rely on PyTorch. PyTorch will be faster! <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5 üìù: Use an MLP to classify the CIFAR-10 dataset\n",
    "\n",
    "Take the Jupyter notebook from exercise 2 (classification with a linear classifier) and modify it to classify the CIFAR-10 dataset with an MLP. <br>\n",
    "In that notebook there is a `CIFAR10` class that defines a PyTorch Module already: \n",
    "```python\n",
    "class CIFAR10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(inputSize, numClasses)\n",
    "        ...\n",
    "```\n",
    "\n",
    "Modify or copy the `CIFAR10` class and change it to an MLP. The number of neurons and layers is up to you! <br>\n",
    "Do you expect better or worse performance than with the linear classifier? <br>\n",
    "Report the accuracy of your new network and compare it to the linear model. <br>\n",
    "\n",
    "Note: Training a larger MLP with loads of data will take a while. So choose your hyperparameters wisely! :)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a05e4fa746b81761c76a645b508c0f51cdd970f4b4b50ae36c6a73f9a174174"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
