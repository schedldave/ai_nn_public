{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise NN/02 - Neural Networks\n",
    "\n",
    "Contact: Dr. David C. Schedl <br/>\n",
    "E-Mail: david.schedl@fh-hagenberg.at <br/>\n",
    "Note: this tutorial is geared towards students **experienced in programming** and aims to introduce you to **Backpropagation and NNs**.\n",
    "\n",
    "*Acknowledgements:*\n",
    "This notebook is strongly inspired by the NN-zero-to-here lecture series by Andrej Karpathy, available [here](https://github.com/karpathy/nn-zero-to-hero).\n",
    "\n",
    "\n",
    "## Setup\n",
    "As first step, we need to import the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and import of libraries\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_blobs, make_circles\n",
    "\n",
    "\n",
    "# Let's set the random seed to make this reproducible (the same for everybody).\n",
    "np.random.seed(1337)\n",
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Helpers\n",
    "\n",
    "The following functions will help us to visualize the compute graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{%sdata %.4f | grad %.4f}\" % (f\"{n.label} |\" if n.label else '', n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "This is where the magic happens ... ‚ú® <br>\n",
    "\n",
    "Let's finish backpropagation by implementing the chain rule in our `Value` class. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  \n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._backward = lambda: None # This is new!\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "    \n",
    "    # TODO: implement the backward pass for addition\n",
    "    def _backward():\n",
    "      self.grad += out.grad\n",
    "      other.grad += out.grad\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "    \n",
    "    # TODO: implement the backward pass for multiplication\n",
    "    def _backward():\n",
    "      self.grad += other.data * out.grad\n",
    "      other.grad += self.data * out.grad\n",
    "    out._backward = _backward\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    # TODO: implement the backward pass for **\n",
    "    def _backward():\n",
    "      self.grad += (other * self.data**(other-1)) * out.grad\n",
    "      # we skip the gradient for other, because we (typically) don't need it\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "    return self * other**-1\n",
    "\n",
    "  def __neg__(self): # -self\n",
    "    return self * -1\n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "    return self + other\n",
    "\n",
    "  def tanh(self):\n",
    "\n",
    "    out = Value( math.tanh(self.data), (self,), 'tanh')\n",
    "\n",
    "    # TODO: implement the backward pass for tanh\n",
    "    def _backward():\n",
    "      # 1 - f(x)^2\n",
    "      local_gradient = (1 - out.data**2)\n",
    "      self.grad += out.grad * local_gradient\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out  \n",
    "\n",
    "  def relu(self):\n",
    "    out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "    # TODO: implement the backward pass for ReLU\n",
    "    def _backward():\n",
    "      self.grad += out.grad if self.data > 0 else 0\n",
    "    out._backward = _backward\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self, ), 'exp')\n",
    "    \n",
    "    # TODO: implement the backward pass for exp\n",
    "    def _backward():\n",
    "      self.grad += out.grad * out.data\n",
    "    out._backward = _backward\n",
    "    \n",
    "    return out\n",
    "  \n",
    "  \n",
    "  def backward(self):\n",
    "    \n",
    "    # get a topographic sort of the graph for backprop\n",
    "    topo = []\n",
    "    visited = set()\n",
    "    def build_topo(v):\n",
    "      if v not in visited:\n",
    "        visited.add(v)\n",
    "        for child in v._prev:\n",
    "          build_topo(child)\n",
    "        topo.append(v)\n",
    "    build_topo(self)\n",
    "\n",
    "    # set the gradient of all nodes to 0\n",
    "    for n in topo:\n",
    "      n.grad = 0.0\n",
    "    \n",
    "    self.grad = 1.0 # set the gradient of the last node to 1\n",
    "\n",
    "    # TODO: finish the backward pass!\n",
    "    for n in reversed(topo):\n",
    "      n._backward()\n",
    "\n",
    "\n",
    "# We want to compute the gradients by calling backward()!\n",
    "\n",
    "x = Value(-2.0); x.label='x'\n",
    "y = Value(5); y.label='y'\n",
    "z = Value(-4.0); z.label='z'\n",
    "\n",
    "q = x + y; q.label = 'q (x+y)'\n",
    "o = q*z; o.label='o'\n",
    "\n",
    "o.backward() # THE magic (should) happen here!\n",
    "\n",
    "# gradients\n",
    "# o.grad = 1.0\n",
    "# q.grad = -4.0\n",
    "# x.grad = -4.0\n",
    "# y.grad = -4.0\n",
    "# z.grad = 3.0\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backprop of Task 1 (previously by hand)\n",
    "The calculations you did by hand in the previous task are now implemented in the `backward` method of the `Value` class. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your birthday here (will be used to generate random numbers)\n",
    "d, m, y = 29, 2, 1980\n",
    "\n",
    "# init the random number generator\n",
    "import random\n",
    "random.seed(d+m+y); r_range = (1, 9)\n",
    "\n",
    "x1 = Value(random.randint(*r_range)); x1.label='x1'\n",
    "x2 = Value(random.randint(*r_range)); x2.label='x2'\n",
    "x3 = Value(random.randint(*r_range)); x3.label='x3'\n",
    "w1 = Value(random.randint(*r_range)); w1.label='w1'\n",
    "w2 = Value(random.randint(*r_range)); w2.label='w2'\n",
    "w3 = Value(random.randint(*r_range)); w3.label='w3'\n",
    "b = Value(random.randint(*r_range));  b.label='b'\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label='x1w1'\n",
    "x2w2 = x2*w2; x2w2.label='x2w2'\n",
    "x3w3 = x3*w3; x3w3.label='x3w3'\n",
    "sums = (x1w1 + x2w2) + (x3w3 + b); sums.label='sum'\n",
    "o = sums * Value(0.25); o.label='o'\n",
    "o.label='o'\n",
    "\n",
    "\n",
    "# Swinging the magic wand: \"Retrorsum Propagatum!\" ... the gradients should appear!\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Neuron with an Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b') # magic number that makes gradients nicer\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "# the tanh activation function \n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "o.backward()\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same in PyTorch!\n",
    "\n",
    "Let's look at the example above implemented in PyTorch. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x1 = torch.Tensor([2.0]).double()                ; x1.requires_grad = True\n",
    "x2 = torch.Tensor([0.0]).double()                ; x2.requires_grad = True\n",
    "w1 = torch.Tensor([-3.0]).double()               ; w1.requires_grad = True\n",
    "w2 = torch.Tensor([1.0]).double()                ; w2.requires_grad = True\n",
    "b = torch.Tensor([6.8813735870195432]).double()  ; b.requires_grad = True # magic number that makes gradients nicer\n",
    "n = x1*w1 + x2*w2 + b\n",
    "o = torch.tanh(n)\n",
    "\n",
    "print(\"o =\", o.data.item())\n",
    "o.backward()\n",
    "\n",
    "print('--- gradients ---')\n",
    "print('x2.grad =', x2.grad.item())\n",
    "print('w2.grad =', w2.grad.item())\n",
    "print('x1.grad =', x1.grad.item())\n",
    "print('w1.grad =', w1.grad.item())\n",
    "# if everything is correct, the gradients should be the same as in our implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Let's get serious now! <br>\n",
    "With our Value class we will implement Neurons and a Multi-Layer Perceptron (MLP) with Layers. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Neuron:\n",
    "  \n",
    "  def __init__(self, nin):\n",
    "    self.w = [Value(random.uniform(-1,1)) for _ in range(nin)]\n",
    "    self.b = Value(random.uniform(-1,1))\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # TODO: get the Neuron working  \n",
    "    act = sum((wi*xi for wi, xi in zip(self.w, x)), self.b)\n",
    "    return act.tanh()    \n",
    "    \n",
    "  \n",
    "  def parameters(self):\n",
    "    return self.w + [self.b]\n",
    "\n",
    "class Layer:\n",
    "  \n",
    "  def __init__(self, nin, nout):\n",
    "    self.neurons = [Neuron(nin) for _ in range(nout)]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # TODO: get the Layer working\n",
    "    outs = [n(x) for n in self.neurons] # apply each neuron to the input\n",
    "    return outs if len(outs) > 1 else outs[0] # return a list if there are multiple outputs or a single value otherwise\n",
    "\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for neuron in self.neurons for p in neuron.parameters()]\n",
    "\n",
    "class MLP:\n",
    "  \n",
    "  def __init__(self, nin, nouts):\n",
    "    sz = [nin] + nouts\n",
    "    self.layers = [Layer(sz[i], sz[i+1]) for i in range(len(nouts))]\n",
    "  \n",
    "  def __call__(self, x):\n",
    "    # TODO: get the MLP working\n",
    "    for layer in self.layers: # apply the layers one after each other\n",
    "      x = layer(x) \n",
    "    return x\n",
    "\n",
    "  \n",
    "  def parameters(self):\n",
    "    return [p for layer in self.layers for p in layer.parameters()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the MLP with a single 3D input like so:\n",
    "\n",
    "ü§î Why does `n` change with every rerun?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [2.0, 3.0, -1.0]\n",
    "n = MLP(3, [4, 4, 1])\n",
    "print(n(x)) # changes every time you run it, WHY?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically we will use multiple inputs, so let's create an array of inputs and desired targets (e.g. for classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [\n",
    "  [2.0, 3.0, -1.0], # example 1\n",
    "  [3.0, -1.0, 0.5], # example 2\n",
    "  [0.5, 1.0, 1.0], # example 3\n",
    "  [1.0, 1.0, -1.0], # example 4\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0] # desired targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to compute a loss function to evaluate the performance of our network. <br>\n",
    "Furthermore, the loss is the basis for backpropagation. <br>\n",
    "\n",
    "ü§î What happens if you change the learning rate or the number of iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "N = 20 # iterations\n",
    "\n",
    "for k in range(N):\n",
    "  \n",
    "  # forward pass\n",
    "  ypred = [n(x) for x in xs]\n",
    "  loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "  \n",
    "  # set gradients to zero\n",
    "  for p in n.parameters():\n",
    "    p.grad = 0.0\n",
    "  loss.backward() # backward pass\n",
    "  \n",
    "  # update\n",
    "  for p in n.parameters():\n",
    "    p.data += -learning_rate * p.grad\n",
    "  \n",
    "  print(k, loss.data)\n",
    "  \n",
    "print( \"========\" )\n",
    "print( \"predictions: \" + str(ypred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's train a network to classify points (like Tensorflow Playground)\n",
    "\n",
    "We will at first generate some data using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make up a dataset\n",
    "X, y = make_moons(n_samples=100, noise=0.1)\n",
    "#X, y = make_circles(n_samples=100, noise=0.05)\n",
    "\n",
    "y = y*2 - 1 # make y be -1 or 1\n",
    "# visualize in 2D\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n",
    "plt.show()\n",
    "print(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize a model \n",
    "model = MLP(2, [16, 16, 1]) # 2-layer neural network\n",
    "print(\"number of parameters\", len(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def loss(batch_size=None):\n",
    "    \n",
    "    # TODO: use mini-batch\n",
    "    Xb, yb = X, y\n",
    "    inputs = [list(map(Value, xrow)) for xrow in Xb]\n",
    "    \n",
    "    # forward the model to get scores\n",
    "    scores = list(map(model, inputs))\n",
    "    \n",
    "    # svm \"max-margin\" loss\n",
    "    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n",
    "    data_loss = sum(losses) * (1.0 / len(losses))\n",
    "    # L2 regularization\n",
    "    alpha = 1e-4\n",
    "    reg_loss = alpha * sum((p*p for p in model.parameters()))\n",
    "    # final loss\n",
    "    total_loss = data_loss + reg_loss\n",
    "    \n",
    "    # besides loss we also care about accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n",
    "    return total_loss, sum(accuracy) / len(accuracy)\n",
    "\n",
    "total_loss, acc = loss()\n",
    "print(total_loss, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "\n",
    "K = 10 # number of iterations\n",
    "\n",
    "# optimization\n",
    "for k in range(K):\n",
    "    \n",
    "    # forward\n",
    "    total_loss, acc = loss(batch_size=batch_size)\n",
    "    \n",
    "    # backward\n",
    "    #model.zero_grad()\n",
    "    total_loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    learning_rate = 1.0 - 0.9*k/K\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "    \n",
    "    if k % 1 == 0:\n",
    "        print(f\"step {k} loss {total_loss.data}, accuracy {acc*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize decision boundary (similar to tensorflow playground)\n",
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "inputs = [list(map(Value, xrow)) for xrow in Xmesh]\n",
    "scores = list(map(model, inputs))\n",
    "Z = np.array([s.data > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks in PyTorch\n",
    "\n",
    "Let's also set up the same MLP as above in PyTorch. <br>\n",
    "\n",
    "Previously our model was defined in code as:\n",
    "```python\n",
    "model = MLP(2, [16, 16, 1]) # 2-layer neural network\n",
    "```\n",
    "Our MLP had 337 parameters. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP in PyTorch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class TorchMLP(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h1 = torch.tanh(self.fc1(x))\n",
    "        h2 = torch.tanh(self.fc2(h1))\n",
    "        o = self.fc3(h2)\n",
    "        return o\n",
    "\n",
    "torch_model = TorchMLP()\n",
    "\n",
    "print(torch_model)\n",
    "# print number of parameters\n",
    "print(sum(p.numel() for p in torch_model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial accuracy (RANDOM)\n",
    "preds = torch_model(torch.Tensor(X)).data.numpy()\n",
    "accuracy = [(yi > 0) == (scorei > 0) for yi, scorei in zip(y, preds)]\n",
    "acc = sum(accuracy) / len(accuracy)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft marigin loss \n",
    "loss_fun = F.soft_margin_loss # NOTE: this is not exactly the same as the one above, but it's close ;)\n",
    "optimizer = torch.optim.SGD(torch_model.parameters(), \n",
    "    lr=.5, # learning rate\n",
    "    weight_decay=1e-5 # L2 regularization\n",
    ")\n",
    "batch_size = 100\n",
    "\n",
    "# optimization\n",
    "for k in range(1000):\n",
    "\n",
    "    # use mini-batch\n",
    "    if batch_size is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "        \n",
    "    # forward\n",
    "    scores = torch_model(torch.from_numpy(Xb).float())\n",
    "    loss = loss_fun(scores, torch.from_numpy(yb).reshape(-1,1).float())\n",
    "    # L2 regularization is in the optimizer, now!!\n",
    "    \n",
    "    # backward\n",
    "    loss.backward()\n",
    "    \n",
    "    # update (sgd)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "    # also get accuracy\n",
    "    accuracy = [(yi > 0) == (scorei.data.item() > 0) for yi, scorei in zip(yb, scores)]\n",
    "    acc = sum(accuracy) / len(accuracy)\n",
    "\n",
    "    if k % 100 == 0:\n",
    "        print(f\"step {k} loss {loss.data}, accuracy {acc*100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize decision boundary (similar to tensorflow playground)\n",
    "\n",
    "h = 0.25\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
    "scores = torch_model(torch.from_numpy(Xmesh).float()).data.numpy()\n",
    "Z = np.array([s > 0 for s in scores])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 üìù: Play with the data and the network\n",
    " \n",
    "Change the MLP (number of layers and number of neurons per layer) and see how it affects the decision boundary. \n",
    "Switch between the `make_circles` and `make_moons` function for dataset generation and see how your network performs. <br>\n",
    "Be careful with parameters such as the learning rate, number of iterations, and  batch size. <br>\n",
    "\n",
    "Try to answer the following questions:\n",
    "- Is there any overfitting? How would you see it? <br>\n",
    "- How many parameters do the networks have and how does that affect performance (accuracy and timing)?\n",
    "- What is the simplest and most complex network you can train to classify the data? Can it get too complex?\n",
    "\n",
    "Hints: You can use our Value class for computations or rely on PyTorch. PyTorch will be (much) faster! <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 üìù: Use an MLP to classify the CIFAR-10 dataset\n",
    "\n",
    "Take the Jupyter notebook from the previous exercise (classification with a linear classifier) and modify it to classify the CIFAR-10 dataset with an MLP. <br>\n",
    "In that notebook there is a `CIFAR10` class that defines a PyTorch Module already: \n",
    "```python\n",
    "class CIFAR10(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(inputSize, numClasses)\n",
    "        ...\n",
    "```\n",
    "\n",
    "Modify or copy the `CIFAR10` class and change it to an MLP. The number of neurons and layers is up to you! <br>\n",
    "Do you expect better or worse performance than with the linear classifier? <br>\n",
    "Report the accuracy of your new network and compare it to the linear model. <br>\n",
    "\n",
    "Note: Training a larger MLP with loads of data will take a while. So choose your hyperparameters wisely! :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 üìù: Use an MLP to classify depth indents on our textile sensor. \n",
    "\n",
    "We have a dataset of depth indents on our textile sensor at random locations. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# We download training and test data from GitHub. Note: there is no validation set. \n",
    "!curl -o \"test.csv\" \"https://raw.githubusercontent.com/splendiferousnoctifer/textileTouchSensor/main/trained_models/depthMergedSensor_test.csv\" --silent\n",
    "!curl -o \"train.csv\" \"https://raw.githubusercontent.com/splendiferousnoctifer/textileTouchSensor/main/trained_models/depthMergedSensor_train.csv\" --silent\n",
    "\n",
    "# Load the training and testing datasets\n",
    "test_df = pd.read_csv('train.csv')\n",
    "train_df = pd.read_csv('test.csv')\n",
    "\n",
    "# Merge the 0 and 10 indent_depths (they are hard to distinguish)\n",
    "train_df[train_df['state'] == 10.0] = 0.0\n",
    "test_df[test_df['state'] == 10.0] = 0.0\n",
    "\n",
    "feature_list = [ 'sensor0',\t'sensor2',\t'sensor3',\t'sensor5',\t'sensor7',\t'sensor8',\t'sensor9',\t'sensor10',\t'sensor11'] # we don't use merged_sensors\n",
    "indent_levels = test_df['state'].unique()  # 0, 10 (optional), 15, 20\n",
    "indent_levels.sort()\n",
    "print(indent_levels)\n",
    "\n",
    "# convert the state into classes\n",
    "train_df['class'] = train_df['state'].apply(lambda x: np.where(indent_levels == x)[0][0])\n",
    "test_df['class'] = test_df['state'].apply(lambda x: np.where(indent_levels == x)[0][0])\n",
    "\n",
    "# show some stats about the training data\n",
    "sample_sizes = np.asarray(train_df['state'].value_counts())\n",
    "print(train_df['state'].value_counts())\n",
    "\n",
    "test_df[:15] # show a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Constants\n",
    "LEARNING_RATE = 0.02\n",
    "WEIGHT_DECAY = 1e-3  # L2 Regularization\n",
    "BATCH_SIZE = 32\n",
    "NUM_ITERATIONS = 10000\n",
    "\n",
    "# Load the training data\n",
    "X = train_df[feature_list].values\n",
    "# One-Hot Encode the target labels\n",
    "y = F.one_hot(torch.from_numpy(train_df['class'].values), len(indent_levels))\n",
    "\n",
    "# Load the validation data. Similar as for train_df\n",
    "X_val = test_df[feature_list].values\n",
    "y_val = F.one_hot(torch.from_numpy(test_df['class'].values), len(indent_levels))\n",
    "\n",
    "class LinearClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A model for linear classification tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        \"\"\"\n",
    "        Define how to perform forward pass.\n",
    "        \"\"\"\n",
    "        return self.linear(xb)\n",
    "    \n",
    "# Initialize the model\n",
    "model = LinearClassifier(X.shape[1], len(indent_levels))\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feed-forward neural network with two hidden layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, xb):\n",
    "        \"\"\"\n",
    "        Define the forward pass.\n",
    "        \"\"\"\n",
    "        xb = F.relu(self.layer1(xb)) \n",
    "        xb = F.relu(self.layer2(xb))\n",
    "        xb = self.layer3(xb)\n",
    "        return xb\n",
    "\n",
    "# Initialize the model\n",
    "model = NeuralNetwork(X.shape[1], len(indent_levels))\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "#optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY) # try with the Adam optimizer\n",
    "\n",
    "# Optimization\n",
    "for k in range(NUM_ITERATIONS):\n",
    "    # Create mini-batch\n",
    "    if BATCH_SIZE is None:\n",
    "        Xb, yb = X, y\n",
    "    else:\n",
    "        ri = np.random.permutation(X.shape[0])[:BATCH_SIZE]\n",
    "        Xb, yb = X[ri], y[ri]\n",
    "\n",
    "    # Convert numpy arrays to tensors\n",
    "    Xb_t = torch.from_numpy(Xb).float()\n",
    "    yb_t = (yb).long()\n",
    "\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Perform forward pass and calculate loss\n",
    "    output = model(Xb_t)\n",
    "    loss = criterion(output, torch.max(yb_t, 1)[1]) # CrossEntropyLoss expects class indices, not one-hot\n",
    "    \n",
    "    # Perform backward pass and update weights\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    _, actual = torch.max(yb_t, 1)\n",
    "    correct = (predicted == actual).sum().item()\n",
    "    accuracy = correct / BATCH_SIZE\n",
    "\n",
    "    # Print progress every 100 steps\n",
    "    if k % 100 == 0:        \n",
    "        # Validation Part\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            X_val_t = torch.from_numpy(X_val).float()\n",
    "            y_val_t = y_val.long()\n",
    "\n",
    "            output_val = model(X_val_t)\n",
    "            loss_val = criterion(output_val, torch.max(y_val_t, 1)[1]) \n",
    "\n",
    "            _, predicted_val = torch.max(output_val, 1)\n",
    "            _, actual_val = torch.max(y_val_t, 1)\n",
    "            val_correct = (predicted_val == actual_val).sum().item()\n",
    "            val_accuracy = val_correct / X_val.shape[0]\n",
    "\n",
    "            print(f\"Iteration {k} - Training Loss: {loss.item()}, Training Accuracy: {accuracy*100:.2f}%, Validation Loss: {loss_val.item()}, Validation Accuracy: {val_accuracy*100:.2f}%\")\n",
    "\n",
    "        model.train()  # Set model back to train mode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a05e4fa746b81761c76a645b508c0f51cdd970f4b4b50ae36c6a73f9a174174"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
