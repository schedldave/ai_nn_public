{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excercise NN/01 - Neural Networks\n",
    "\n",
    "Contact: Dr. David C. Schedl <br/>\n",
    "E-Mail: david.schedl@fh-hagenberg.at <br/>\n",
    "Note: this tutorial is geared towards students **experienced in programming** and aims to introduce you to **Backpropagation and NNs**.\n",
    "\n",
    "*Acknowledgements:*\n",
    "This notebook is strongly inspired by the NN-zero-to-here lecture series of Andrej Karpathy, available [here](https://github.com/karpathy/nn-zero-to-hero).\n",
    "\n",
    "## Setup\n",
    "As first step, we need to import the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and import of libraries\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing Gradients\n",
    "Let's start simple with the quadratic function $f(x) = 3x^2 - 4x + 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "  return 3*x**2 - 4*x + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it in the range $[-5, 5]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.arange(-5, 5, 0.25)\n",
    "ys = f(xs)\n",
    "plt.plot(xs, ys, label='f(x)')\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of this function is $\\frac{df(x)}{dx} = 6x - 4$.\n",
    "\n",
    "The minimum ($0 = \\frac{df(x)}{dx}$) is at $x = \\frac{4}{6} = \\frac{2}{3}$.\n",
    "\n",
    "Let's numerically derive the function and let's look at some values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.000001\n",
    "x = 2/3 # test with -4, 0, 2/3 and 4\n",
    "(f(x + h) - f(x))/h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î Can you also derive $df(x)/df$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Gradient Descent\n",
    "\n",
    "With the gradient we can now implement a simple gradient descent algorithm, which iteratively updates the value of $x$ in the direction of the negative gradient.\n",
    "The parameters are the learning rate (often denoted as $\\alpha$) and the number of iterations $N$.\n",
    "\n",
    "ü§î What happens if you change the learning rate or the number of iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple gradient descent\n",
    "\n",
    "x = -4\n",
    "_xs = [x]\n",
    "learning_rate = 0.1\n",
    "N = 10 # number of iterations\n",
    "for i in range(N):\n",
    "    df = (f(x + h) - f(x))/h\n",
    "    x = x - learning_rate * df\n",
    "    _xs.append(x)\n",
    "\n",
    "print(f\"x' reached: {x} after {N} iterations and should be 2/3!\")\n",
    "plt.plot(xs, ys, label='f(x)') # plot the function\n",
    "plt.plot(_xs, f(np.array(_xs)), 'r.', label='x\\'') # plot the path our gradient descent took\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Complex Functions\n",
    "An example with a more complex function and multiple parameters:\n",
    "$f(x, y, z) = (x + y) * z$. <br>\n",
    "This example with the exact same parameters is used on the slides: \n",
    "$ x = -2, y = 5, z = -4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get more complex\n",
    "x = -2.0\n",
    "y = 5.0\n",
    "z = -4.0\n",
    "def f(x,y,z): \n",
    "    return (x+y)*z\n",
    "print(f(x,y,z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function has three parameters ($x,y,z$) and therefore multiple gradients are computed: $\\frac{df(x,y,z)}{dx}, \\frac{df(x,y,z)}{dy}, \\frac{df(x,y,z)}{dz}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.0001\n",
    "\n",
    "# inputs\n",
    "x = -2.0\n",
    "y = 5.0\n",
    "z = -4.0\n",
    "\n",
    "f1 = f(x,y,z)\n",
    "f2 = f(x,y,z)+h # slope -> ~  1\n",
    "# f2 = f(x+h,y,z) # slope -> ~ -4\n",
    "# f2 = f(x,y+h,z) # slope -> ~ -4\n",
    "# f2 = f(x,y,z+h) # slope -> ~  3\n",
    "\n",
    "print('f1', f1)\n",
    "print('f2', f2)\n",
    "print('slope', (f2 - f1)/h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 üìù: Derivative on sheet and paper\n",
    "\n",
    "Hand in a sheet/picture showing the manual derivative of the equation above: $f(x, y, z) = (x + y) * z$.\n",
    "\n",
    "What are the analytical gradients of $f(x,y,z)$ with respect to $x, y, z$ ($\\frac{df(x,y,z)}{dx}, \\frac{df(x,y,z)}{dy}, \\frac{df(x,y,z)}{dz}$)?\n",
    "\n",
    "Verify your results by comparing it to the numerical gradients (above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towards Backpropagation\n",
    "This is where the magic will happen ... ‚ú® <br>\n",
    "\n",
    "## The Value class\n",
    "First we'll implement a Value class that will help us to keep track of the compute graph, values and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "  \n",
    "  def __init__(self, data, _children=(), _op='', label=''):\n",
    "    self.data = data\n",
    "    self.grad = 0.0\n",
    "    self._prev = set(_children)\n",
    "    self._op = _op\n",
    "    self.label = label\n",
    "\n",
    "  def __repr__(self):\n",
    "    return f\"Value(data={self.data})\"\n",
    "  \n",
    "  def __add__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data + other.data, (self, other), '+')\n",
    "       \n",
    "    return out\n",
    "\n",
    "  def __mul__(self, other):\n",
    "    other = other if isinstance(other, Value) else Value(other)\n",
    "    out = Value(self.data * other.data, (self, other), '*')\n",
    "      \n",
    "    return out\n",
    "  \n",
    "  def __pow__(self, other):\n",
    "    assert isinstance(other, (int, float)), \"only supporting int/float powers for now\"\n",
    "    out = Value(self.data**other, (self,), f'**{other}')\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def __rmul__(self, other): # other * self\n",
    "    return self * other\n",
    "\n",
    "  def __truediv__(self, other): # self / other\n",
    "    return self * other**-1\n",
    "\n",
    "  def __neg__(self): # -self\n",
    "    return self * -1\n",
    "\n",
    "  def __sub__(self, other): # self - other\n",
    "    return self + (-other)\n",
    "\n",
    "  def __radd__(self, other): # other + self\n",
    "    return self + other\n",
    "\n",
    "  def tanh(self):\n",
    "    x = self.data\n",
    "    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)\n",
    "    out = Value(t, (self, ), 'tanh')\n",
    "    \n",
    "    return out\n",
    "\n",
    "  def relu(self):\n",
    "    out = Value(0 if self.data < 0 else self.data, (self,), 'ReLU')\n",
    "\n",
    "    return out\n",
    "  \n",
    "  def exp(self):\n",
    "    x = self.data\n",
    "    out = Value(math.exp(x), (self, ), 'exp')\n",
    "      \n",
    "    return out\n",
    "\n",
    "# a quick working example\n",
    "print(Value(1) + Value(4))\n",
    "print(Value(2) * Value(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization Helpers\n",
    "\n",
    "The following functions will help us to visualize the compute graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "def trace(root):\n",
    "  # builds a set of all nodes and edges in a graph\n",
    "  nodes, edges = set(), set()\n",
    "  def build(v):\n",
    "    if v not in nodes:\n",
    "      nodes.add(v)\n",
    "      for child in v._prev:\n",
    "        edges.add((child, v))\n",
    "        build(child)\n",
    "  build(root)\n",
    "  return nodes, edges\n",
    "\n",
    "def draw_dot(root):\n",
    "  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right\n",
    "  \n",
    "  nodes, edges = trace(root)\n",
    "  for n in nodes:\n",
    "    uid = str(id(n))\n",
    "    # for any value in the graph, create a rectangular ('record') node for it\n",
    "    dot.node(name = uid, label = \"{%sdata %.2f | grad %.2f}\" % (f\"{n.label} |\" if n.label else '', n.data, n.grad), shape='record')\n",
    "    if n._op:\n",
    "      # if this value is a result of some operation, create an op node for it\n",
    "      dot.node(name = uid + n._op, label = n._op)\n",
    "      # and connect this node to it\n",
    "      dot.edge(uid + n._op, uid)\n",
    "\n",
    "  for n1, n2 in edges:\n",
    "    # connect n1 to the op node of n2\n",
    "    dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "  return dot\n",
    "\n",
    "\n",
    "# Let's visualize a very simple example\n",
    "x = Value(1.0)\n",
    "y = (x * 2 + 1).tanh()\n",
    "draw_dot(y)\n",
    "# ignore grad for now!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use the example from above\n",
    "An example with a more complex function and multiple parameters:\n",
    "$f(x, y, z) = (x + y) * z$. <br>\n",
    "This example with the exact same parameters is used on the slides: \n",
    "$ x = -2, y = 5, z = -4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Value(-2.0); x.label='x'\n",
    "y = Value(5); y.label='y'\n",
    "z = Value(-4.0); z.label='z'\n",
    "\n",
    "q = x + y; q.label = 'q (x+y)'\n",
    "o = q*z; o.label='o'\n",
    "\n",
    "# gradients\n",
    "o.grad = 1.0\n",
    "q.grad = -4.0\n",
    "x.grad = -4.0\n",
    "y.grad = -4.0\n",
    "z.grad = 3.0\n",
    "\n",
    "draw_dot(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 üìù: Backprop on sheet and paper\n",
    "\n",
    "Hand in a picture (hand-drawn or digitally with the code below) showing the gradients for the example below. Document your steps how you got to your result. <br>\n",
    "\n",
    "The example will be different for each student as the random values are different. Enter your birthdate (`d, m, y`) in the code below to get individual values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enter your birthday here (will be used to generate random numbers)\n",
    "d, m, y = 29, 2, 1980\n",
    "\n",
    "# init the random number generator\n",
    "import random\n",
    "random.seed(d+m+y); r_range = (1, 9)\n",
    "\n",
    "x1 = Value(random.randint(*r_range)); x1.label='x1'\n",
    "x2 = Value(random.randint(*r_range)); x2.label='x2'\n",
    "x3 = Value(random.randint(*r_range)); x3.label='x3'\n",
    "w1 = Value(random.randint(*r_range)); w1.label='w1'\n",
    "w2 = Value(random.randint(*r_range)); w2.label='w2'\n",
    "w3 = Value(random.randint(*r_range)); w3.label='w3'\n",
    "b = Value(random.randint(*r_range));  b.label='b'\n",
    "\n",
    "x1w1 = x1*w1; x1w1.label='x1w1'\n",
    "x2w2 = x2*w2; x2w2.label='x2w2'\n",
    "x3w3 = x3*w3; x3w3.label='x3w3'\n",
    "sums = (x1w1 + x2w2) + (x3w3 + b); sums.label='sum'\n",
    "o = sums * Value(0.25); o.label='o'\n",
    "o.label='o'\n",
    "draw_dot(o)\n",
    "\n",
    "# Todo: what are the gradients?\n",
    "# o.grad = 1.0 ... and so on\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
